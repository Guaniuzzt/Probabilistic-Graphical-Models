{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eecs491 Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR from images using CRF & word analysis\n",
    "\n",
    "Shuxiang Zhu sxz529\n",
    "\n",
    "Zetao Zhu zsz690"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will build a basic optical character recognition system(OCR) to recognize words from images. OCR system is quite useful in different fields. For example, recognizing words form a letter written decades years ago. OCR will involve knowledge we have learned in this semester like Markov Network and some knowledge in Machine Learning to train data. In addition, we will also combine OCR with Naïve Bayes Classifier to classifier the property of a combination of several words. Detail processes and analysis of this project will show in following parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CRF model, there are two kinds of variables: the hidden variables that we want to model and those that are always observed. In the case of OCR, we want to model the character assignments (such as ‘a’ or ‘c’), and we always observe the character images, which are arrays of pixel values. Typically, the unobserved variables are denoted by Y and the observed variables are denoted by X. The CRF seeks to model P(Y |X), the conditional distribution over character assignments given the observed images. The structure of the model is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/OCR.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Random Field Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the training set consists of n words. The image of the i-th word can be represented as $X_i = (x^i_1,... x^i_m) (a list)$, where m is the number of letters in the word, and xij is a 128 dimensional vector that represents its j-th letter image. To ease notation, we simply assume all words have m letters, and the model extends naturally to the general case where the length of word varies. The sequence label of a word is encoded as $y^i = (y^i_1,..., y^i_m)$, where $y^i_j \\in Y := {1, 2,...,26}$ represents the label of the j-th letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this notation, the Conditional Random Field (CRF) model for this task is a sequence shown as follows, and the probabilistic model for a word $y = (y_1,...,y_m)$ given its image $X = (x_1,...,x_m)$\n",
    "can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/function1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<..>denotes inner product between vectors. Two groups of parameters are used here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letter-wise discriminant weight vector wy 2 R128 for each possible label y $\\in$ Y;  \n",
    "Transition weight matrix T which is sized 26-by-26. $T_{ij}$is the weight associated with the letter pair of the i-th and j-th letter in the alphabet. For example $T_{1,9}$ is the weight for pair (`a', `i'), and $T_{24;2}$ is for the pair (`x', `b'). In general Tij $\\not=$ Tji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these parameters (e.g. by learning from data), the model above can be used to predict the sequence label (i.e. word) for a new word image $ X^* = (x^*_1,...,x^*_m)$ via:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Conditional Random Fields "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training set $\\{X^i, |y^i\\}^i_{i=1}$, the parameters$\\{w_y : y\\in Y\\}$ and T can be estimated by maximum a posteriori over the conditional distribution above, or equivalently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/function2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $C > 0$ is a trade-off weight that balances log-likelihood and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the filepath,read train data or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(pattern):\n",
    "    data = []\n",
    "    files = glob.glob(pattern)\n",
    "    for name in files:\n",
    "        f = open(name, 'r')\n",
    "        label = next(f).strip()\n",
    "        features = np.asarray([[int(b) for b in c.split()] for c in f])\n",
    "        data.append((label, features))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_model(model_dir):\n",
    "    theta = []\n",
    "    files = ['state-params.txt', 'transition-params.txt']\n",
    "    for name in files:\n",
    "        f = open(model_dir + '/' + name, 'r')\n",
    "        params = [[float(d) for d in line.split()] for line in f]\n",
    "        theta.append(params)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write model generated into .txt in ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_model(theta, state_file, trans_file):\n",
    "    files = [state_file, trans_file]\n",
    "    for (params, name) in zip(theta, files):\n",
    "        f = open(name, 'w')\n",
    "        for row in params:\n",
    "            for cell in row:\n",
    "                f.write(str(cell)+\" \")\n",
    "            f.write(\"\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(predictions, data):\n",
    "    true_count = sum([sum([c1 == c2 for c1, c2 in zip(prediction, label)])\n",
    "                      for prediction, (label, _) in zip(predictions, data)])\n",
    "    total_count = sum([len(prediction) for prediction in predictions])\n",
    "    accuracy = 1.0 * true_count / total_count\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a  $w \\times k$ numpy array of node potentials, where w is the word length and k is the size of the alphabet.\n",
    "\n",
    "Parameters:\n",
    "- features, a $w \\times k$ numpy array of feature vectors, where n is the length of the feature vector\n",
    "- state_params, a $w \\times k$ numpy array of state parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def node_potentials(features, state_params):\n",
    "    return np.dot(features, np.transpose(state_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the clique potentials of a single clique.\n",
    "Returns a k x k numpy array\n",
    "\n",
    "Parameters:\n",
    "- node_factor1, a k-dimensional numpy array of node potentials\n",
    "- node_factor2, a k-dimensional numpy array of node potentials or a None object\n",
    "- trans_params, a $k \\times k$ numpy array of transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clique_potentials(node_factor1, node_factor2, trans_params):\n",
    "    psi = trans_params + node_factor1[:, np.newaxis]\n",
    "    if node_factor2 is not None:\n",
    "        psi += node_factor2\n",
    "\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the clique potentials of the entire chain.\n",
    "\n",
    "Returns a $(w-1) \\times k \\times k$ numpy array, where w is the word length and k is the size of the alphabet.\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- features, a $w \\times n$ numpy array, where n is the length of the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clique_tree_potentials(theta, features):\n",
    "    state_params, trans_params = theta\n",
    "    phi = node_potentials(features, state_params)\n",
    "\n",
    "    # Include the potentials of the last two nodes in the same clique\n",
    "    cliques = [(node, None) for node in phi[:-2]] + [(phi[-2], phi[-1])]\n",
    "\n",
    "    psi = [clique_potentials(n1, n2, trans_params) for n1, n2 in cliques]\n",
    "\n",
    "    return np.array(psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the (backward messages, forward messages) tuple. Each messages is a $(w-2) \\times k$ numpy array, where w is the word length and k is the size of the alphabet.\n",
    "\n",
    "Parameter:\n",
    "- psi, a $ (w-1) \\times k \\times k$ numpy array of clique tree potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_product_messages(psi):\n",
    "    bwd = []\n",
    "    prev_msgs = np.zeros(psi.shape[1])\n",
    "    for clique in psi[:0:-1]:\n",
    "        msg = scipy.misc.logsumexp(clique + prev_msgs, axis=1)\n",
    "        bwd.append(msg)\n",
    "        prev_msgs += msg\n",
    "\n",
    "    # Forward messages\n",
    "    fwd = []\n",
    "    prev_msgs = np.zeros(psi.shape[1])\n",
    "    for clique in psi[:-1]:\n",
    "        msg = scipy.misc.logsumexp(clique + prev_msgs[:, np.newaxis], axis=0)\n",
    "        fwd.append(msg)\n",
    "        prev_msgs += msg\n",
    "\n",
    "    return (np.array(bwd), np.array(fwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a numpy array of size $ (w-1) \\times k \\times k$\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- features, a w x n numpy array of feature vectors, where n is the length of the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beliefs(theta, features):\n",
    "    psi = clique_tree_potentials(theta, features)\n",
    "    delta_bwd, delta_fwd = sum_product_messages(psi)\n",
    "\n",
    "    k = delta_fwd.shape[1]\n",
    "    delta_fwd = np.concatenate(([np.zeros(k)], delta_fwd))\n",
    "    delta_bwd = np.concatenate((delta_bwd[::-1], [np.zeros(k)]))\n",
    "    beta = psi + delta_fwd[:, :, np.newaxis] + delta_bwd[:, np.newaxis]\n",
    "\n",
    "    return np.array(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the pairwise marginal probabilities. Returns a numpy array of size $(w-1) \\times k \\times k$\n",
    "\n",
    "Parameter:\n",
    "- beta, $a (w-1) \\times k \\times k$ numpy array of log belief tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_prob(beta):\n",
    "    return np.exp(beta - scipy.misc.logsumexp(beta, axis=(1,2))[:, np.newaxis, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the singleton marginal probabilities.\n",
    "\n",
    "Parameter:\n",
    "- pairwise_p, a numpy array of size $ (w-1) \\times k \\times k$ of pairwise marginal probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_prob(pairwise_p):\n",
    "    p = np.sum(pairwise_p, axis=2)\n",
    "    q = np.sum(pairwise_p[-1], axis=0) # Last character in the word\n",
    "\n",
    "    return np.concatenate((p, q[np.newaxis, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the joint probability of the label given singleton marginal probabilities.\n",
    "\n",
    "Returns a scalar.\n",
    "\n",
    "Parameters:\n",
    "- single_p, a $w \\times k$ numpy array of singleton marginal probabilities, where n is the word length and k is the size of the alphabet\n",
    "- label, a list of character labels\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joint_prob(single_p, label, alphabet):\n",
    "    p = [np.log(marginal[alphabet.index(c)]) for (c, marginal) in zip(label, single_p)]\n",
    "    return np.sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function to minimize.\n",
    "\n",
    "Returns the negative average log likelihood of theta given the data.\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- data, a list of (word_label, word_features) tuples\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(theta, data, alphabet):\n",
    "    # If flattened, reshape theta into a list of state parameter table of size n x k\n",
    "    # and transition parameter table of size k x k,\n",
    "    # where n is the length of the feature vector and k is the size of the alphabet\n",
    "    if len(theta) != 2:\n",
    "        k = len(alphabet)      # number of possible character labels\n",
    "        n = len(data[0][1][0]) # length of feature vector\n",
    "        mid = k * n\n",
    "\n",
    "        state_params = np.reshape(theta[:mid], (k, n))\n",
    "        trans_params = np.reshape(theta[mid:], (k, k))\n",
    "        theta = [state_params, trans_params]\n",
    "\n",
    "    p = []\n",
    "    for label, features in data:\n",
    "        beta = beliefs(theta, features)\n",
    "        pairwise_p = pairwise_prob(beta)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        p.append(joint_prob(single_p, label, alphabet))\n",
    "\n",
    "    return -np.sum(p)/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a flattened k x n numpy array, where k is the size of the alphabet and n is the length of the feature vector.\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- data, a list of (word_label, word_features) tuples\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def state_gradient(theta, data, alphabet):\n",
    "    # Initialize a state gradient table of size k x n with zeros\n",
    "    gradient = np.zeros((len(alphabet), len(data[0][1][0])))\n",
    "\n",
    "    for label, features in data:\n",
    "        beta = beliefs(theta, features)\n",
    "        pairwise_p = pairwise_prob(beta)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        for v, c, p in zip(features, label, single_p):\n",
    "            for i in range(gradient.shape[0]): # possible labels\n",
    "                for j in range(gradient.shape[1]): # features\n",
    "                    indicator = 0\n",
    "                    if c == alphabet[i]:\n",
    "                        indicator = 1\n",
    "                    gradient[i][j] += (indicator - p[i]) * v[j]\n",
    "    \n",
    "    gradient /= len(data)\n",
    "\n",
    "    return np.ndarray.flatten(np.negative(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a flattened $k \\times k$ numpy array, where k is the size of the alphabet.\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- data, a list of (word_label, word_features) tuples\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_gradient(theta, data, alphabet):\n",
    "    # Initialize a transition gradient table of size k x k with zeros\n",
    "    gradient = np.zeros((len(alphabet), len(alphabet)))\n",
    "\n",
    "    for label, features in data:\n",
    "        beta = beliefs(theta, features)\n",
    "        pairwise_p = pairwise_prob(beta)\n",
    "        #label_pairs = zip([None] + label, label + [None])[1:-1]\n",
    "        label_pairs = zip(label[0: -1], label[1:])\n",
    "\n",
    "        for (label1, label2), p in zip(label_pairs, pairwise_p):\n",
    "            for i in range(gradient.shape[0]):\n",
    "                for j in range(gradient.shape[1]):\n",
    "                    indicator = 0\n",
    "                    if label1 == alphabet[i] and label2 == alphabet[j]:\n",
    "                        indicator = 1\n",
    "                    gradient[i][j] += indicator - p[i][j]\n",
    "\n",
    "    gradient /= len(data)\n",
    "\n",
    "    return np.ndarray.flatten(np.negative(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a flattened numpy array of the [feature_gradient, transition_gradient] list.\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- data, a list of (word_label, word_features) tuples\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood_prime(theta, data, alphabet):\n",
    "    # Reshape flattened theta into a list of k x n state parameter table and\n",
    "    # k x k transition parameter table, where k is the size of the alphabet and\n",
    "    # n is the length of the feature vector. Both parameter tables are numpy arrays.\n",
    "    k = len(alphabet)      # number of possible character labels\n",
    "    n = len(data[0][1][0]) # length of feature vector\n",
    "    mid = k * n\n",
    "\n",
    "    state_params = np.reshape(theta[:mid], (k, n))\n",
    "    trans_params = np.reshape(theta[mid:], (k, k))\n",
    "    theta = [state_params, trans_params]\n",
    "\n",
    "    return np.concatenate((state_gradient(theta, data, alphabet), transition_gradient(theta, data, alphabet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a list of predicted characters of a word.\n",
    "\n",
    "Parameters:\n",
    "- single_p, a w x k numpy array of singleton marginal probabilities, where w is the word length and k is the size of the alphabet\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_word(single_p, alphabet):\n",
    "    \n",
    "    indices = np.argmax(single_p, axis=1)\n",
    "\n",
    "    return [alphabet[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a list of predictions, where each prediction is a list of predicted character labels of a word.\n",
    "\n",
    "Parameters:\n",
    "- theta, a [state_params, trans_params] list\n",
    "- data, a list of (word_label, word_features) tuples\n",
    "- alphabet, a list of all possible character labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(theta, data, alphabet):\n",
    "    predictions = []\n",
    "    for _, features in data:\n",
    "        beta = beliefs(theta, features)\n",
    "        pairwise_p = pairwise_prob(beta)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        predictions.append(predict_word(single_p, alphabet))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the learned [state_params, trans_params] list, where each parameter table is a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited-memory BFGS (L-BFGS or LM-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, alphabet, maxiter, log):\n",
    "\n",
    "    # Initialize state and transition parameter tables with zeros\n",
    "    state_params = np.ndarray.flatten(np.zeros((len(alphabet), len(data[0][1][0]))))\n",
    "    trans_params = np.ndarray.flatten(np.zeros((len(alphabet), len(alphabet))))\n",
    "    theta = np.concatenate([state_params, trans_params])\n",
    "\n",
    "    # Learn by minimizing the negative average log likelihood\n",
    "    theta, fmin, _ = fmin_l_bfgs_b(likelihood, theta, fprime=likelihood_prime, args=(data, alphabet), maxiter=maxiter, disp=log)\n",
    "\n",
    "    # Write training summary to log\n",
    "    if log > 0:\n",
    "        print (\"Training data size:\", len(data))\n",
    "        print (\"Value of likelihood function at minimum:\", np.exp(-fmin))\n",
    "\n",
    "    k = len(alphabet)\n",
    "    n = len(data[0][1][0])\n",
    "    mid = k * n\n",
    "    state_params = np.reshape(theta[:mid], (k, n))\n",
    "    trans_params = np.reshape(theta[mid:], (k, k))\n",
    "\n",
    "    return [state_params, trans_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 400 data cases\n",
      "Training data size: 400\n",
      "Value of likelihood function at minimum: 0.999995440283\n",
      "Saving model to ./model\n"
     ]
    }
   ],
   "source": [
    "train_pattern = './data/train*.txt'\n",
    "model_dir = './model'\n",
    "alphabet = 'etainoshrd'\n",
    "max_iter = 50\n",
    "log = 1\n",
    "\n",
    "alphabet = list(alphabet)\n",
    "# Read training data\n",
    "data = read_data(train_pattern)\n",
    "if log > 0:\n",
    "\tprint('Successfully read', len(data), 'data cases')\n",
    "\n",
    "# Train the model\n",
    "model = train(data, alphabet, max_iter, log)\n",
    "\n",
    "# Save the model\n",
    "if log > 0:\n",
    "\tprint ('Saving model to', model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "\tos.makedirs(model_dir)\n",
    "state_file = model_dir + \"/state-params.txt\"\t\n",
    "trans_file = model_dir + \"/transition-params.txt\"\n",
    "print_model(model, state_file, trans_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data from ./data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(theta, data, alphabet, print_tags, print_score):\n",
    "    predictions = predict(theta, data, alphabet)\n",
    "    predicted = []\n",
    "\n",
    "    if print_tags:\n",
    "        for word in predictions:\n",
    "            print (''.join(word))\n",
    "            predicted.append(''.join(word))\n",
    "\n",
    "    if print_score:\n",
    "        print ('predict score: ', score(predictions, data))\n",
    "        \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree\n",
      "net\n",
      "dorated\n",
      "hands\n",
      "attended\n",
      "ease\n",
      "rhode\n",
      "the\n",
      "nasa\n",
      "tea\n",
      "rotation\n",
      "raised\n",
      "adi\n",
      "erosion\n",
      "dot\n",
      "noise\n",
      "sins\n",
      "ratiot\n",
      "inoia\n",
      "sort\n",
      "sentor\n",
      "titan\n",
      "error\n",
      "asteroids\n",
      "sinai\n",
      "addresses\n",
      "trend\n",
      "one\n",
      "seasons\n",
      "this\n",
      "the\n",
      "assist\n",
      "oata\n",
      "rhirds\n",
      "artist\n",
      "eddie\n",
      "its\n",
      "tenor\n",
      "rest\n",
      "asd\n",
      "shots\n",
      "rests\n",
      "edison\n",
      "ash\n",
      "andes\n",
      "and\n",
      "riata\n",
      "added\n",
      "intended\n",
      "his\n",
      "shorter\n",
      "nes\n",
      "sean\n",
      "raios\n",
      "torah\n",
      "that\n",
      "indian\n",
      "ninetern\n",
      "deer\n",
      "serson\n",
      "tonnes\n",
      "diana\n",
      "ttat\n",
      "atheist\n",
      "sente\n",
      "and\n",
      "eastern\n",
      "aid\n",
      "and\n",
      "terrorist\n",
      "isis\n",
      "thnt\n",
      "series\n",
      "eroeed\n",
      "sad\n",
      "and\n",
      "tender\n",
      "sheet\n",
      "santos\n",
      "theodore\n",
      "entert\n",
      "santa\n",
      "nnd\n",
      "that\n",
      "traneition\n",
      "treaties\n",
      "sarah\n",
      "antonio\n",
      "drain\n",
      "seeded\n",
      "the\n",
      "resistant\n",
      "those\n",
      "taste\n",
      "needs\n",
      "retreated\n",
      "denote\n",
      "start\n",
      "rhind\n",
      "iooted\n",
      "hand\n",
      "and\n",
      "interested\n",
      "assertion\n",
      "distant\n",
      "rise\n",
      "residents\n",
      "sister\n",
      "theorists\n",
      "desert\n",
      "needed\n",
      "resorts\n",
      "trend\n",
      "ernest\n",
      "disasters\n",
      "teresa\n",
      "nad\n",
      "ieities\n",
      "restore\n",
      "nead\n",
      "theories\n",
      "third\n",
      "shan\n",
      "east\n",
      "arrest\n",
      "dna\n",
      "retained\n",
      "taere\n",
      "that\n",
      "seas\n",
      "readers\n",
      "that\n",
      "the\n",
      "hire\n",
      "strait\n",
      "eritrea\n",
      "thirreenth\n",
      "read\n",
      "address\n",
      "rises\n",
      "terrain\n",
      "norions\n",
      "this\n",
      "destinrtion\n",
      "diet\n",
      "these\n",
      "indtana\n",
      "assassinated\n",
      "sends\n",
      "deteriorated\n",
      "the\n",
      "hair\n",
      "neither\n",
      "hanoi\n",
      "deaths\n",
      "has\n",
      "roads\n",
      "retired\n",
      "stared\n",
      "ratio\n",
      "riot\n",
      "der\n",
      "addis\n",
      "horse\n",
      "tensions\n",
      "ions\n",
      "tended\n",
      "are\n",
      "haiti\n",
      "the\n",
      "ain\n",
      "shortened\n",
      "hidten\n",
      "rand\n",
      "aided\n",
      "interior\n",
      "dorset\n",
      "area\n",
      "artists\n",
      "sanitation\n",
      "anne\n",
      "dresden\n",
      "reason\n",
      "rio\n",
      "entered\n",
      "tier\n",
      "hired\n",
      "the\n",
      "henin\n",
      "identities\n",
      "doha\n",
      "not\n",
      "the\n",
      "reeo\n",
      "sahara\n",
      "head\n",
      "resided\n",
      "tin\n",
      "hide\n",
      "hosts\n",
      "predict score:  0.9686635944700461\n"
     ]
    }
   ],
   "source": [
    "test_pattern = './data/test*.txt'\n",
    "test_pattern1 = './data2/testa*.txt'\n",
    "test_pattern2 = './data2/testb*.txt'\n",
    "test_pattern3 = './data2/testc*.txt'\n",
    "test_pattern4 = './data2/testd*.txt'\n",
    "\n",
    "model_dir = './model'\n",
    "alphabet = 'etainoshrd'\n",
    "\n",
    "print_tags = 1\n",
    "print_score = 1\n",
    "\n",
    "# Open feature and transition params\n",
    "theta = read_model(model_dir)\n",
    "\n",
    "# Read test data\n",
    "data = read_data(test_pattern)\n",
    "data1 = read_data(test_pattern1)\n",
    "data2 = read_data(test_pattern2)\n",
    "data3 = read_data(test_pattern3)\n",
    "data4 = read_data(test_pattern4)\n",
    "\n",
    "predicted0 = test(theta, data, alphabet, print_tags, print_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclustion  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the OCR is shown above. All test words have been transfered from matrixs to real word.The accuracy of the OCR is 0.968,which is pretty high. But there a still some words like donated has been recognized as dornated, that is because the letter n is very similar with r and our train data is not enough. If we got more train data, the accuracy will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayse Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, predict some word images using CRF.\n",
    "\n",
    "<img src=\"data2/test_img100.png\">\n",
    "\n",
    "<img src=\"data2/test_img102.png\">\n",
    "\n",
    "<img src=\"data2/test_img137.png\">\n",
    "\n",
    "<img src=\"data2/test_img118.png\">\n",
    "\n",
    "<img src=\"data2/test_img160.png\">\n",
    "\n",
    "<img src=\"data2/test_img161.png\">\n",
    "\n",
    "<img src=\"data2/test_img21.png\">\n",
    "\n",
    "<img src=\"data2/test_img183.png\">\n",
    "\n",
    "<img src=\"data2/test_img174.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hands\n",
      "ease\n",
      "ash\n",
      "predict score:  1.0\n",
      "error\n",
      "terrorist\n",
      "isis\n",
      "predict score:  1.0\n",
      "needs\n",
      "disasters\n",
      "predict score:  1.0\n",
      "traneition\n",
      "predict score:  0.9\n"
     ]
    }
   ],
   "source": [
    "predicted1 = test(theta, data1, alphabet, print_tags, print_score)\n",
    "predicted2 = test(theta, data2, alphabet, print_tags, print_score)\n",
    "predicted3 = test(theta, data3, alphabet, print_tags, print_score)\n",
    "predicted4 = test(theta, data4, alphabet, print_tags, print_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to  Bayesian theory, when handling a classification problem, the probability of an eignvale belongs to a certain class is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/Bayes.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to complete probability formula, the formula also could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"report/complete.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use naive Bayse classifier to label words sequence.The inputs of the classifier will be the words we get from OCR system we just introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish this naive Bayse classifier, the first thing we need to do is to setup a dataset. The postingList has 12 rows, each one has been taged 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "\n",
    "    postingList=[['ease', 'nasa', 'tea', 'and', 'donated', 'hands'],\n",
    "                 ['terrorist', 'noise', 'error', 'isis', 'oath', 'tenor', 'park', 'stupid'],\n",
    "                 ['needs', 'interested', 'sister', 'ernest', 'horse', 'I', 'love', 'him'],\n",
    "                 ['and', 'sins', 'his', 'odd', 'that', 'hand', 'had'],\n",
    "                 ['noise', 'addresses', 'ash', 'shorter', 'that', 'how', 'to', 'stop', 'him'],\n",
    "                 ['assassinated', 'deaths', 'shortened', 'not', 'net', 'desert'],\n",
    "                 ['not', 'head', 'are', 'terrain', 'and', 'neither'],\n",
    "                 ['disasters', 'near', 'address', 'destination', 'has', 'desert'],\n",
    "                 ['the', 'are', 'rise', 'near', 'east', 'destination'],\n",
    "                 ['neither', 'roads', 'residents', 'isis', 'and','those'],\n",
    "                 ['has', 'neither', 'roads', 'readers', 'and', 'that'],\n",
    "                 ['are', 'noise', 'not', 'that', 'roads', 'error']]\n",
    "    classVec = [0,1,0,1,0,1,0,1,0,1,0,1]    # 1 represent contains special words\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    # Create a nonrepeating wordset\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    "                   \n",
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calucating conditional probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainBayes(trainMatrix,trainCategory):\n",
    "\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    # Initiate\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords)#\n",
    "    p0Denom = 2.0; p1Denom = 2.0 #\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "\n",
    "    p1Vect = np.log(p1Num/p1Denom)#change to log()\n",
    "    p0Vect = np.log(p0Num/p0Denom)#change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n",
    "def classifyBayes(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting input and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hands', 'ease', 'ash'] classified as:  0\n",
      "['error', 'terrorist', 'isis'] classified as:  1\n",
      "['needs', 'disasters'] classified as:  1\n",
      "the word: traneition is not in my Vocabulary!\n",
      "['traneition'] classified as:  0\n"
     ]
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "\n",
    "p0V,p1V,pAb = trainBayes(np.array(trainMat), np.array(listClasses))\n",
    "\n",
    "testEntry1 = predicted1\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry1))\n",
    "print(testEntry1,'classified as: ',classifyBayes(thisDoc,p0V,p1V,pAb))\n",
    "\n",
    "testEntry2 = predicted2\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry2))\n",
    "print(testEntry2,'classified as: ',classifyBayes(thisDoc,p0V,p1V,pAb))\n",
    "\n",
    "testEntry3 = predicted3\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry3))\n",
    "print(testEntry3,'classified as: ',classifyBayes(thisDoc,p0V,p1V,pAb))\n",
    "\n",
    "testEntry4 = predicted4\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry4))\n",
    "print(testEntry4,'classified as: ',classifyBayes(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result of Naive Bayse Classifier, the performance is good. If we choose all words which are all have shown in catalog 0, the tag of this word sequence is 0.If we choose all words which are all have shown in catalog 1, the tag of this word sequence is 1.If we choose some words have shown in catalog 0 and some in catalog 1, the tag of this word sequence  will depend on the weight of property of words. If the word has not shown in postinglist, it will display \"traneition is not in my Vocabulary!\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
